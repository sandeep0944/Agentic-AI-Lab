{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Self-Reflective Multi-Agent RAG System\n",
                "\n",
                "This notebook goes through each part of the system step by step.\n",
                "\n",
                "The basic idea is this: instead of doing the usual retrieve-and-generate in one shot,\n",
                "we have separate agents that each handle one piece of the puzzle:\n",
                "\n",
                "1. **Planner** — breaks down complex questions into smaller parts\n",
                "2. **Retriever** — finds relevant chunks from the paper\n",
                "3. **Answer agent** — generates an answer from the context\n",
                "4. **Critic** — checks if the answer is actually good\n",
                "5. **Revision agent** — fixes the answer if the critic isn't happy\n",
                "6. **Memory** — remembers past interactions\n",
                "\n",
                "There's also a query complexity detector that decides whether the planner\n",
                "is even needed (no point decomposing simple questions).\n",
                "\n",
                "Everything runs locally — no API keys required."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Setup\n",
                "\n",
                "Make sure you've installed the requirements:\n",
                "```\n",
                "pip install -r requirements.txt\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "# need the project root on the path for imports to work\n",
                "root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
                "if root not in sys.path:\n",
                "    sys.path.insert(0, root)\n",
                "\n",
                "print(f\"Project root: {root}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Loading the PDF\n",
                "\n",
                "First step is getting the text out of the PDF. We use pypdf for this.\n",
                "Put your PDF at `data/research_paper.pdf` or change the path below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.pdf_loader import load_pdf, show_pdf_info\n",
                "\n",
                "PDF_PATH = os.path.join(root, 'data', 'research_paper.pdf')\n",
                "\n",
                "if not os.path.exists(PDF_PATH):\n",
                "    print(f\"No PDF found at {PDF_PATH}\")\n",
                "    print(\"Put a research paper there or change PDF_PATH\")\n",
                "else:\n",
                "    pdf_info = load_pdf(PDF_PATH)\n",
                "    show_pdf_info(pdf_info)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Chunking\n",
                "\n",
                "We split the text into smaller pieces. Each chunk becomes one vector in our search index.\n",
                "\n",
                "The overlap (100 chars) is important — without it you lose context at the edges of chunks.\n",
                "I tested with and without overlap and retrieval quality was noticeably worse without it.\n",
                "\n",
                "Chunk size of 600 chars is a compromise. Too small and each chunk lacks context.\n",
                "Too big and the embedding becomes too vague."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.chunking import chunk_text, show_chunk_stats\n",
                "\n",
                "if 'pdf_info' in dir():\n",
                "    chunks = chunk_text(pdf_info['text'], chunk_size=600, overlap=100)\n",
                "    show_chunk_stats(chunks)\n",
                "    \n",
                "    # show one chunk so we can see what they look like\n",
                "    if chunks:\n",
                "        c = chunks[0]\n",
                "        print(f\"\\nChunk 0 (start={c['start']}, end={c['end']}):\")\n",
                "        print(c['text'][:300] + '...')\n",
                "else:\n",
                "    # no PDF? use some dummy text\n",
                "    dummy = (\n",
                "        \"Machine learning is a subset of AI that builds systems that learn from data. \"\n",
                "        \"The methodology involves training on labeled datasets. Key limitations include \"\n",
                "        \"the need for lots of labeled data and overfitting risk. \"\n",
                "    ) * 10\n",
                "    chunks = chunk_text(dummy, chunk_size=600, overlap=100)\n",
                "    show_chunk_stats(chunks)\n",
                "    print('\\n(using dummy text since no PDF was loaded)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Embeddings\n",
                "\n",
                "Each chunk gets turned into a 384-dimensional vector using the all-MiniLM-L6-v2 model.\n",
                "The idea is that chunks about similar topics will have vectors that are close together.\n",
                "So when we embed a question and search for nearest neighbors, we get relevant chunks.\n",
                "\n",
                "The model is about 80MB and downloads on the first run. After that its cached."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.embeddings import get_model, make_embeddings, build_index\n",
                "\n",
                "emb_model = get_model()\n",
                "vecs = make_embeddings(chunks, emb_model)\n",
                "\n",
                "print(f\"\\nShape: {vecs.shape}\")\n",
                "print(f\"Each chunk is now a {vecs.shape[1]}-dim vector\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. FAISS Index\n",
                "\n",
                "We store the vectors in a FAISS IndexFlatL2 for searching. Its brute-force (checks every\n",
                "vector on each query) which is O(n*d) per search. For a single paper with maybe 100-200\n",
                "chunks thats basically instant. You'd need approximate methods for larger collections."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "index = build_index(vecs)\n",
                "print(f\"Index has {index.ntotal} vectors\")\n",
                "print(f\"Memory: ~{index.ntotal * 384 * 4 / 1024:.1f} KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Retrieval\n",
                "\n",
                "Now we can search. Given a question, we embed it and find the closest chunks.\n",
                "Lower L2 distance = more similar. If the distance is too high we flag it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.retrieval import find_top_chunks, build_context, show_results\n",
                "\n",
                "test_q = \"What is the methodology used in this paper?\"\n",
                "found = find_top_chunks(test_q, index, chunks, emb_model, top_k=3)\n",
                "show_results(found)\n",
                "\n",
                "ctx = build_context(found)\n",
                "print(f\"\\nContext length: {len(ctx)} chars\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Query Complexity Detection\n",
                "\n",
                "Before we even plan anything, we check if the query actually needs planning.\n",
                "\n",
                "Simple queries like \"what dataset was used?\" don't need to be decomposed.\n",
                "Complex ones like \"compare methodology and limitations\" do.\n",
                "\n",
                "The detector looks for keywords: \"compare\", \"difference\", \"and\" (connecting topics),\n",
                "\"advantages and disadvantages\", \"limitations\", etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.planner_agent import check_complexity\n",
                "\n",
                "test_queries = [\n",
                "    \"What dataset was used?\",\n",
                "    \"Explain the methodology and limitations.\",\n",
                "    \"What are the advantages and disadvantages?\",\n",
                "    \"Compare results with baseline.\",\n",
                "]\n",
                "\n",
                "for q in test_queries:\n",
                "    c = check_complexity(q)\n",
                "    tag = 'COMPLEX' if c['is_complex'] else 'SIMPLE'\n",
                "    print(f\"[{tag}] {q}\")\n",
                "    print(f\"  reason: {c['reason']}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Planner Agent\n",
                "\n",
                "For complex queries, the planner splits them into focused subtasks.\n",
                "\n",
                "This really matters for retrieval quality. If you search for \"methodology and limitations\"\n",
                "as one query the embedding is somewhere in between both topics. By splitting, each subtask\n",
                "gets its own focused search."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.planner_agent import plan_query, show_plan\n",
                "\n",
                "test = [\n",
                "    \"What is the problem statement?\",\n",
                "    \"Explain the methodology and limitations.\",\n",
                "    \"What are the contributions compared to existing work?\",\n",
                "]\n",
                "\n",
                "for q in test:\n",
                "    plan = plan_query(q)\n",
                "    show_plan(plan)\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Answer Generation\n",
                "\n",
                "We use flan-t5-base to generate answers. Its a 250M parameter model from Google\n",
                "that runs on CPU. Not amazing, but free and it handles Q&A decently.\n",
                "\n",
                "The prompt tells it to only use the provided context. This helps against hallucination\n",
                "but doesn't eliminate it — thats why we have the critic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.answer_agent import build_answer, show_answer\n",
                "\n",
                "ans = build_answer(test_q, ctx)\n",
                "show_answer(ans)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 10. Critic Agent\n",
                "\n",
                "The critic checks the answer on a few things:\n",
                "- Is it long enough?\n",
                "- Is it grounded in the context (not hallucinated)?\n",
                "- Does it actually answer the question?\n",
                "\n",
                "I use heuristic checks as the main scoring signal because honestly, asking flan-t5-base\n",
                "to evaluate its own output is not very reliable. The heuristics catch the obvious stuff\n",
                "and the LLM feedback adds some qualitative notes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.critic_agent import evaluate, show_eval\n",
                "\n",
                "ev = evaluate(ans['answer'], ctx, test_q)\n",
                "show_eval(ev)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 11. Revision Agent\n",
                "\n",
                "If the score is below 7, we try to improve the answer.\n",
                "It takes the original answer + the critic's feedback and generates a new version.\n",
                "Max 2 rounds to avoid wasting time.\n",
                "\n",
                "With flan-t5-base the improvements are sometimes small. A bigger model would\n",
                "make this loop way more effective."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.revision_agent import run_revision_loop, show_revision\n",
                "\n",
                "if ev['needs_revision']:\n",
                "    rev = run_revision_loop(ans['answer'], ctx, test_q, evaluate)\n",
                "    show_revision(rev)\n",
                "else:\n",
                "    print('Score is fine, no revision needed.')\n",
                "    rev = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 12. Memory\n",
                "\n",
                "The memory module just keeps a log of everything that happened.\n",
                "Useful for follow-up questions and debugging."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.memory import Memory\n",
                "\n",
                "mem = Memory()\n",
                "\n",
                "final = rev['final_answer'] if rev else ans['answer']\n",
                "mem.save(\n",
                "    query=test_q,\n",
                "    answer=ans['answer'],\n",
                "    final_answer=final,\n",
                "    critic_score=ev['score'],\n",
                "    feedback=ev['feedback'],\n",
                "    revisions=rev['rounds'] if rev else 0,\n",
                ")\n",
                "\n",
                "mem.show()\n",
                "print(f\"\\nContext for follow-ups:\\n{mem.get_recent()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 13. Full Pipeline\n",
                "\n",
                "Now lets run everything together using the orchestrator from main.py."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.main import answer_query\n",
                "\n",
                "# reuse what we already loaded\n",
                "if 'pdf_info' in dir():\n",
                "    pipe = {\n",
                "        'chunks': chunks,\n",
                "        'index': index,\n",
                "        'model': emb_model,\n",
                "        'memory': Memory(),\n",
                "        'pdf_info': pdf_info,\n",
                "    }\n",
                "    print('Using previously loaded components.')\n",
                "else:\n",
                "    print('No PDF loaded. Go back to step 2.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# test query 1 - problem statement\n",
                "if 'pipe' in dir():\n",
                "    r1 = answer_query(\"What is the problem statement of this paper?\", pipe)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# test query 2 - compound query, tests the planner\n",
                "if 'pipe' in dir():\n",
                "    r2 = answer_query(\"Explain the methodology and limitations.\", pipe)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# test query 3 - contribution comparison\n",
                "if 'pipe' in dir():\n",
                "    r3 = answer_query(\"What are the key contributions compared to existing work?\", pipe)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# check what's in memory\n",
                "if 'pipe' in dir():\n",
                "    pipe['memory'].show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 14. Results Summary\n",
                "\n",
                "| Query | Type | Subtasks | Chunks | Critic Score | Revisions |\n",
                "|-------|------|----------|--------|-------------|----------|\n",
                "| Problem statement | Simple | 1 | 3 | (fill in) | (fill in) |\n",
                "| Methodology & limitations | Complex | 3 | 6+ | (fill in) | (fill in) |\n",
                "| Contributions vs existing | Complex | 2+ | 3+ | (fill in) | (fill in) |\n",
                "\n",
                "Fill in the scores after running with your PDF.\n",
                "\n",
                "### What I noticed\n",
                "\n",
                "- The planner correctly picks up compound queries and splits them\n",
                "- Retrieval quality depends a lot on how well the PDF text maps to common terms\n",
                "- The critic is decent at catching short or ungrounded answers\n",
                "- Revision helps sometimes, but with a 250M param model the improvements are limited\n",
                "- The complexity detector saves time on simple queries by skipping unnecessary planning"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 15. Limitations\n",
                "\n",
                "- flan-t5-base is small. Answers are often short and lacking detail.\n",
                "- Context window is ~512 tokens so we have to truncate retrieved text.\n",
                "- The planner uses keyword matching which misses some edge cases.\n",
                "- FAISS brute force search is O(n*d) — fine here, not scalable.\n",
                "- pypdf can't handle scanned PDFs or complex layouts well.\n",
                "- The critic's heuristics are approximate at best.\n",
                "- Hallucination is still possible despite the grounding prompts."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}